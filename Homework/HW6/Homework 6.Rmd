---
title: "Homework 6"
author: "Gianni Spiga"
date: "2023-02-24"
output:
  pdf_document: default
  html_document: default
---

# Problem Set 4

## Question 13

```{r, echo = F, message = F}
library(knitr)
library(car)
library(boot)
```


```{r, echo = F}
mela <- read.table("melanoma.txt", header = TRUE)
B <- 2000

# Confidence Elipseses via Bootstap 
beta.est <- function(data, ind) {
  res <- glm(totalincidence ~ ., data = data[ind, ], family = poisson())
  coef(res)
}
boot.res <- boot(mela, beta.est, R=B)

alpha = 0.05

# 95% bootstrap confidence interval for beta_1
ci.B1 <- sort(boot.res$t[,2])[round(c((B+1)*alpha/2, (B+1)*(1-alpha/2)))]
# 95% bootstrap confidence interval for beta_2
ci.B2 <- sort(boot.res$t[,3])[round(c((B+1)*alpha/2, (B+1)*(1-alpha/2)))]

# 95% and 99% bootstrap confidence regions for (beta_1, beta_2)
dataEllipse(
  boot.res$t[, 2],
  boot.res$t[, 3],
  xlab = "year coefficient",
  ylab = "sunspot coefficient",
  cex = .3,
  levels = c(.95, .99)
)

kable(rbind(ci.B1, ci.B2))
```


```{r, echo = F}
glm.mela <- glm(totalincidence ~ years + sunspotnumber, data = mela, family = poisson())
sum.mela <- summary(glm.mela)
#sum.mela

invSigma <- solve(vcov(glm.mela)[2:3,2:3])
beta <- coef(glm.mela)[2:3]

beta.test <- function(data, ind){
res <- glm(totalincidence ~ ., data=data[ind,], family=poisson())
beta.star <- coef(res)[2:3]
t(beta.star-beta) %*% invSigma %*% (beta.star-beta)
}
boot.test <- boot(mela, beta.test, R=B)
#boot.test

#(sum(as.vector(boot.test$t) > as.vector(t(beta) %*% invSigma %*% beta)) + 1) / (B + 1)
```

When testing the null hypothesis $H_0: \beta_i =0, \ i=1,2$ we find that the coefficient for year is significant at $\alpha=0.05$ and the coefficient for sunspot is not. Our confidence interval for sunspots ($\beta_2$) contains zero, which we can see from both the interval and the ellipse. These results line up with our previous ones in Homework 2 when we performed formal tests. In testing for the overall regression effect with bootstrap, we get a p-value of 0.0004, matching with the similar conclusion of problem 7 in problem set 1 as well. 

## Problem 14

```{r, echo = F, warning = F}
pima <- read.table("pima-indians-diabetes.dat", sep = ",")

names(pima) <- c("NPreg", "PGC", "DBP", "Tricep", "Serum.Insulin", "BMI", "Pedigree", "Age", "Diabetic")
# Description of data on page 146
#pima <- pima[,-c(1, 7)]

# Carry out fit with binomial regression 
pima.fit <- glm(Diabetic ~ ., data = pima, family = "binomial")
#summary(pima.fit)


#Confidence intervals by asymptotic normality
pima.coef <- summary(pima.fit)$coef
pima.ci <- cbind(pima.coef[,1] - 1.96*pima.coef[,2], pima.coef[,1] + 1.96*pima.coef[,2])


beta.est.bin <- function(data, ind) {
  res <- glm(Diabetic ~ ., data = data[ind, ], family = binomial())
  coef(res)
}

# Confidence intervals via bootstrap
boot.pima <- boot(pima, beta.est.bin, R = B)
#boot.pima$t

pima.ci.boot <- data.frame(LB = numeric(0), UB = numeric(0))
for (i in 1:ncol(boot.pima$t)){
  pima.ci.boot[nrow(pima.ci.boot)+1, ] <- rbind(pima.ci, sort(boot.pima$t[,i])[round(c((B+1)*alpha/2, (B+1)*(1-alpha/2)))])
}
kable(pima.ci)
kable(pima.ci.boot)
```

Though some of the confidence intervals for the slopes differ above, we can see that they are very similar to each other. The CI's for the standard MLE approximation is on the left, and the bootstrap intervals are on the right. Regardless of the width of the interval, all of them lead to the same conclusion for rejecting or failing to reject their respective null hypothesis. 

# Problem Set 5

## Question 1

One of the advantages of the log link is that it stabilizes the variance of data with a constant coefficient of variation. By doing so, one could run ordinary least squares on the log-transformed data. However, the intercepts would be biased by the offset -0.5*$v$
 where $v$ is the coefficient of variation. The canonical inverse link is just not as practical.
 
 
## Question 2

### a.) 
$$
E(Y) = E(\mu(\epsilon + 1)) = E(\mu * 1) = \mu \\
Var(Y) = Var(\mu(\epsilon + 1)) = \mu^2 Var(\epsilon) = \mu^2\sigma^2 \\ 
v = \frac{var(y)}{E(y)^2} = \frac{\mu^2\sigma^2}{\mu^2} = \sigma^2
$$
Thus $v$ is constant. 

### b.) 

We could either fit the Gamma regression model to the data, assuming we woul use the log link. However, we could also used the log link and perform an OLS for the data. However, performing this ordinary least suares would be biased by the offset mentioned in question 1. 

## Question 8 

### a.) 

The GLM which would be suitable is the logistic regression model predicting the binary response Y, using the logit link. We could combine samples from the US and Japan and create a column that identifies whether or not a observation is from the United States or Japan. 

### b.) 

Intuitively, the logit link is the best link of choice since it is the canonical link for logistic regression. If there was a different link choice hypothesized, a goodness-of-link test would be appropriate to help identify the best link. 

### c.) 

We would be curious to test that the interaction between whether the woman is from Japan or the US and the variable fat. Since the investigator is curious in testing that the US and Japan are the same, we would test this in our alternative hypothesis. Call the coefficient for the mentioned interaction $\beta_4$, we test:
$$
H_0: \beta_4 \neq 0 \\
H_a: \beta_4 = 0
$$
However, testing if a value is strictly equal to zero is very challenging to perform. Instead we would need to discuss with the investigator if we could expand the hypothesis into a a small interval centered around zero. This way, we would have a better ability to approximate with the hypothesis. 

### d.) 

```{r, echo = F, message = F}
library(lattice)

x <- c(-2, -0.5, 0, 0.5, 2)
# stripplot(x)

x <- data.frame(x,1) ## 1 is your "height"
{
plot(x, type = 'o', pch = '|', ylab = '')
rect(-0.5, 1.2, 0.5,0.8, col = "grey")
}
```

Let the grey region be our alternative hypothesis, and all points outside of this region be our null. Our p-value would be the minimum $\alpha$ level needed to include our point in this interval. In this example, the region is from -0.5 to 0.5, but this arbitrary and can be picked by the investigator at any level.

### e.) 

We could embed a new model with a quadratic term or a another non-linear term. Doing this, we could compare the models via a likelihood ratio test and return the results to the investigator. 

### f.) 

We can imagine the model as the following:
$$
E(Y|X) = f(x)
$$
We could use a GAM with a non-linear function on $f(x)$ to measure the complex relationship of the probability to the expectation of $Y$. However, this would make interpretation complex of the probabilities. Another option would be picking a smoothing kernel estimation to understand the relationship between the predictors and the response, in this case fat and age on whether or not one is diagnosed with breast cancer.
