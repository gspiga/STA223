---
title: "Homework 2"
author: "Gianni Spiga"
date: '2023-01-23'
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---


# Question 6
## a.) 
```{r, echo = FALSE}
library(xtable)
library(knitr)
options(xtable.floating = FALSE)
options(xtable.timestamp = "")
```

```{r, echo = FALSE}
mela <- read.table("melanoma.txt", header = TRUE)
#xtable(head(mela))
glm.mela <- glm(totalincidence ~ years + sunspotnumber, data = mela, family = poisson())
sum.mela <- summary(glm.mela)
#print(xtable(summary(glm.mela)), type = "latex", comment= FALSE)
kable(sum.mela$coefficients)
#kable(summary(glm.mela))
```
## b.) 
```{r,echo = FALSE}
kable(anova(glm.mela, test = "Chisq"))
```
We can see from the $\chi^2$ test that the number of sun spots is not significant at any reasonable $\alpha$, however the year the melanoma have been diagnosed is highly significant. 

## c.) 
We test the following hypothesis for $i = 1,2,3$:
$$
H_0: \beta_i = 0 \\
H_a: \beta_i > 0
$$

```{r,echo = FALSE}
# Intercept
pnorm(-12.643, lower.tail = FALSE)

# Years
pnorm(13.201, lower.tail = FALSE)

# Sunspot number
pnorm(0.957, lower.tail = FALSE)

```

We can see for both the intercept and the years, that we reject our null hypothesis, concluding that we have significant evidence that the number of melanoma increases over the calendar years. However, we do not have evidence to conclude that the number of sunspots increases with the number of melanomas. 

# Question 7
```{r,echo = FALSE}
# Find the variance covariance matrix
mela.vmat <- vcov(glm.mela)

# Find the eigen values 
mela.evals <- eigen(mela.vmat)$values

# get the eigenvalues corresponding to the variables (no intercept)
mela.evals <- mela.evals[2:3]

library(ellipse)
{plot(ellipse(glm.mela, which = c(2,3), level = 0.95))
points(x = c(0), y = c(0), cex = 2, pch = 6, col = "blue")}
```

$$
H_0: \beta_1 = \beta_2 = 0 \\
H_a: \beta_1 \neq \beta_2 \neq 0
$$

We could determine a p-value to test the null hypothesis is the minimum value of alpha for the null hypothesis to be true i.e. the null hypothesis is within the confidence ellipsoid. However, the following ellipsoid below is a $1- \alpha = 0.99999999999$ confidence level. The point $(0,0)$ is encodded to have a blue triangle on it. 

```{r, echo = FALSE}
{plot(ellipse(glm.mela, which = c(2,3), level = 0.99999999999))
points(x = c(0), y = c(0), cex = 2, pch = 6, col = "blue")}
```

However, the point of interest is not within graphing visibility of our ellipsoid. Since we are already at such a small level of $\alpha$, we can conclude that the p-value is very close to 0, leading us to reject this null hypothesis at any reasonable significance level. 

# 8.)
$$
H_0: \beta_1 \leq \beta_2 \\
H_a: \beta_1 > \beta_2 
$$

```{r, echo = FALSE}
### QUESTION 8
x <- seq(0,.1, 0.01); y <- seq(0,.1, 0.01)
{plot(x,y, type = 'l', xlab = "Beta1", ylab = "Beta2")
points(glm.mela$coefficients[2], glm.mela$coefficients[3],cex = 2, pch = 6, col = "blue")
}
```

Since we can see that our observed $\hat{\beta_1} > \hat{\beta_2}$ is below the line $\beta_1 = \beta_2$, we have reason to continue testing our hypothesis. We can then use a chi square test. 

```{r, echo = FALSE}
info <- vcov(glm.mela)[2:3, 2:3]
info

betahat <- c(glm.mela$coefficients[2], glm.mela$coefficients[3])


# We know that Sigma is equal to inverse of finitely estimated information matrix, so the inverse of sigma is the information matrix
chi.stat <- (betahat - c(1,-1)) %*% info %*% (betahat - c(1,-1))

# We print the p-value for the one sided test
pchisq(chi.stat, df = 2, lower.tail = TRUE) / 2
```

We solve for a p-value of $0.0000023$, leading us to reject the null and conclude we have statistically sufficient evidence that $\beta_1 > \beta_2$. 

# Question 9

## a.)
One example of a consistent estimator for $\hat{m}(x)$ is the Kernel density estimator. In the paper, "Consistency of the kernel density estimator - a survey", Weid and Wei√übach prove that the Kernel Density Estimator is is almost sure convergent, which is stronger than convergence in probability, the requirement for consistency of an estimator. 

## b.) 
We can use the quotient type parameter for smoothing. The bias and variance depend since by definition of consistency, we expect as n approached infinity, the variance will approach zero. Because of this, we must keep in mind the trade off there is with the variance and bias when choosing. 

## c.) 


# Question 13
## a.) 
```{r, echo = FALSE}
zmk <- read.table("zmk.txt", header = FALSE)
names(zmk) <- c("Voltage", "Force")

plot(zmk$Voltage, zmk$Force)
library(locfit)

fit1over <- locfit(Force ~ lp(Voltage, nn = 0.1), data = zmk)
fit2over <- locfit(Force ~ lp(Voltage, nn = 0.05), data = zmk)
#summary(fit1)


plot(fit1over,col = "red", main = "Oversmoothing with Smoothing Parameter = 0.1")
points(zmk$Voltage, zmk$Force)
plot(fit2over,col = "red", main = "Oversmoothing with Smoothing Parameter = 0.05")
points(zmk$Voltage, zmk$Force)


# Now we plot with underfitting 
fit1under <- locfit(Force ~ lp(Voltage, nn = 1), data = zmk)
fit2under <- locfit(Force ~ lp(Voltage, nn = 20), data = zmk)
#summary(fit1)


plot(fit1under,col = "red", main = "Undersmoothing with Smoothing Parameter = 2")
points(zmk$Voltage, zmk$Force)
plot(fit2under,col = "red", main = "Undersmoothing with Smoothing Parameter = 20")
points(zmk$Voltage, zmk$Force)
```

## b.)

```{r,echo = FALSE}
fit.good <- locfit(Force ~ lp(Voltage, nn = 0.8), data = zmk)
plot(fit.good, col = "red")
points(zmk$Voltage, zmk$Force)
```

## c.) 
The strategy for finding a good fit is to visualzie the smoothing fits over the points and find the curve that accurately fits the data without fitting too many indivudal points and still showing visual changes in directions of the data. 